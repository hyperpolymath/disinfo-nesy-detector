// SPDX-License-Identifier: AGPL-3.0-or-later
// SPDX-FileCopyrightText: 2024 Hyperpolymath

= NSAI Disinfo Detector Roadmap
:toc: macro
:toc-title: Contents
:toclevels: 2
:sectnums:
:icons: font

Development roadmap for the Neuro-Symbolic AI Disinformation Detector.

toc::[]

== Current State

[cols="1,2,1"]
|===
|Layer |What Exists |Status

|**Orchestration**
|Go service with NATS JetStream consumer, Prometheus metrics, graceful shutdown, AOP hooks
|Complete

|**Message Schema**
|Protobuf definitions for `AnalysisInput`, `NeuralFeatures`
|Complete

|**Neural Inference**
|ONNX wrapper interface (stub implementation returns placeholder scores)
|Stub

|**Symbolic Reasoning**
|Souffle wrapper interface (stub returns "SAFE" verdict)
|Stub

|**Knowledge Graph**
|Dgraph integration interface (stub returns hardcoded facts)
|Stub

|**Infrastructure**
|Containerfile, podman-compose, Helm charts, SaltStack states
|Complete

|**CI/CD**
|Rollback scripts, aspect configuration (Nickel)
|Partial
|===

== Phase 1: Core Implementation

=== 1.1 Neural Model Development

* [ ] Design neural architecture for disinformation detection
** Text encoder (transformer-based)
** Emotion/sentiment classifier
** Visual artifact detector (for image analysis)
* [ ] Train models on labeled disinformation datasets
* [ ] Quantize models to ONNX format
* [ ] Create `neural_models/quantized_model.onnx`
* [ ] Implement real ONNX inference in wrapper

=== 1.2 Datalog Rule System

* [ ] Design Souffle schema for disinformation reasoning
* [ ] Author initial rule set:
** Source credibility rules
** Claim verification rules
** Propagation pattern rules
** Emotional manipulation rules
* [ ] Create `symbolic_logic/rules.dl`
* [ ] Implement Souffle FFI or embedded execution
* [ ] Define neural-to-symbolic threshold mappings

=== 1.3 Knowledge Graph Schema

* [ ] Design Dgraph schema:
** Source nodes (publishers, authors, domains)
** Claim nodes (statements, assertions)
** Evidence nodes (supporting/refuting facts)
** Relationship edges (cites, contradicts, amplifies)
* [ ] Create seeding scripts for known sources
* [ ] Implement real Dgraph client in orchestrator
* [ ] Add GraphQL query layer

== Phase 2: Rust Conversion

Per RSR (Rust Standard Requirements) policy, convert from Go to Rust.

=== 2.1 Rust Foundation

* [ ] Create `Cargo.toml` with dependencies:
+
[source,toml]
----
async-nats = "0.33"
ort = "2.0"
prometheus = "0.13"
prost = "0.12"
tokio = { version = "1", features = ["full"] }
tracing = "0.1"
dgraph-tonic = "0.11"
----

* [ ] Set up project structure (`src/`, `benches/`, `tests/`)
* [ ] Configure `rust-toolchain.toml` (stable + WASM targets)

=== 2.2 Component Migration

[cols="1,2,1"]
|===
|Go Component |Rust Replacement |Priority

|`cmd/main.go`
|`src/main.rs` with tokio async runtime
|High

|`pkg/onnx_wrapper/`
|`src/onnx.rs` using `ort` crate
|High

|`pkg/souffle_wrapper/`
|`src/datalog.rs` using `crepe` macro or Souffle FFI
|High

|`pkg/model_pb/`
|`src/proto/` generated by `prost-build`
|Medium

|Prometheus metrics
|`prometheus` crate with `lazy_static` registries
|Medium
|===

=== 2.3 Rust-Native Datalog

Evaluate options:

* **crepe** - Pure Rust Datalog via procedural macros
* **Souffle FFI** - Call Souffle binary via subprocess
* **ascent** - Rust-embedded Datalog

== Phase 3: Testing & Validation

=== 3.1 Test Infrastructure

* [ ] Unit tests for each component
* [ ] Integration tests with containerized dependencies
* [ ] Property-based testing for Datalog rules
* [ ] Benchmark suite for latency requirements

=== 3.2 Validation Dataset

* [ ] Curate labeled disinformation corpus
* [ ] Establish precision/recall baselines
* [ ] A/B testing framework for rule changes

=== 3.3 Explainability Audit

* [ ] Verify all verdicts include human-readable explanations
* [ ] Trace neural feature -> rule firing -> verdict path
* [ ] Document decision boundaries

== Phase 4: Production Hardening

=== 4.1 Security

* [ ] Input sanitization for all external content
* [ ] Rate limiting on inference endpoints
* [ ] Audit logging with tamper detection
* [ ] Secrets management (HashiCorp Vault integration)

=== 4.2 Scalability

* [ ] Horizontal scaling via NATS consumer groups
* [ ] Model serving with batched inference
* [ ] Dgraph sharding for large knowledge graphs
* [ ] CDN caching for repeated content hashes

=== 4.3 Observability

* [ ] Distributed tracing (OpenTelemetry)
* [ ] Alerting rules for error rate spikes
* [ ] Dashboard for verdict distribution
* [ ] Model drift detection

== Phase 5: Extensions

=== 5.1 Multi-Modal Analysis

* [ ] Audio analysis (speech-to-text + emotion)
* [ ] Video frame sampling
* [ ] Document layout analysis (PDFs)

=== 5.2 Federated Knowledge

* [ ] Cross-instance fact sharing protocol
* [ ] Consensus mechanism for disputed claims
* [ ] Privacy-preserving aggregation

=== 5.3 Active Learning

* [ ] Human-in-the-loop feedback integration
* [ ] Continuous model retraining pipeline
* [ ] Rule suggestion from pattern mining

== Dependencies

=== External Services

|===
|Service |Purpose |Required By

|NATS JetStream
|Message streaming
|Phase 1

|Dgraph
|Knowledge graph
|Phase 1

|ONNX Runtime
|Neural inference
|Phase 1

|Prometheus
|Metrics collection
|Phase 1

|HashiCorp Vault
|Secrets management
|Phase 4
|===

=== Missing Assets

|===
|Asset |Path |Blocked By

|Neural model
|`neural_models/quantized_model.onnx`
|Phase 1.1

|Datalog rules
|`symbolic_logic/rules.dl`
|Phase 1.2

|Dgraph schema
|`dgraph/schema.graphql`
|Phase 1.3
|===

== Open Questions

1. **Neural architecture**: Transformer fine-tuning vs. custom architecture?
2. **Datalog engine**: Embedded (crepe) vs. external (Souffle)?
3. **Knowledge graph source**: Manual curation vs. automated extraction?
4. **Verdict granularity**: Binary (safe/unsafe) vs. multi-class vs. continuous?
5. **Deployment model**: Edge-first vs. cloud-centralized?

== References

* link:RUST_CONVERSION_NEEDED.md[Rust Conversion Requirements]
* link:docs/architecture/README.md[Architecture Overview]
* link:docs/CITATIONS.adoc[Citation Formats]
